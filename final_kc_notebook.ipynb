{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real Estate Investment Opportunities in King County\n",
    "Authors: Anton Smirnov, Tom Chapman"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The King County housing market is one of the hottest in the country, remaining resilient even through the recent decline in demand. For this project, we will utilize linear regression modelling to analyze sales data and provide recommendations to investors looking to get in on the fun."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Problem/Stakeholders\n",
    " \n",
    "We (Atoman Research) are based in King County and working on a project for a real estate investment trust. They would like us to use our area expertise to advise them on what sort of properties are likely to maximize the value of their investment. They have been tight-lipped about their plans for the properties they purchase, so we do not know if they intend to buy and hold, rennovate, demolish and rebuild, etc.\n",
    "\n",
    "King County is an extremely hot real estate market. Zillow's home value index indicates the typical home price is $934,000, which is up 24.5 percent over the past year. Demand for King County real estate is high partly because several large employers in the area that provide highly compensated positions with excellent benefit packages, causing a higher than average number of wealthy buyers. This phenomenon is similar to the Bay Area in California, and the tri-state area surrounding New York City. \n",
    "\n",
    "Because we aim to maximize the value of our all of clients' investments, we aim to provide advice to a broad range of investors. Certainly a higher sale price is appealing, but we must also consider the risk that comes with large individual investments. Expensive homes have a limited pool of buyers, even in a wealthy locale like King County. We aim to identify property features that result in the higheest price per square foot across all properties in our data. This allows our clients to make investment decisions that match their specific fund sizes and risk tolerances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this analysis we utilized a dataset of King County sales records collected over a 12 month period. Although the data dates back to 2014-2015, the age of the data has no impact on the recommendations presented herein. Therefore we treat the data as if it were current. The dataset includes all of the satndard datapoints one would expect when describing home sales. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, median_absolute_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import sklearn.metrics as metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Housing Data\n",
    "simple_housing = pd.read_csv('../data/kc_house_data.csv')\n",
    "simple_housing.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define & remove out-of-scope features of the data\n",
    "complexities = ['id','date', 'view', 'sqft_above', 'sqft_basement', 'yr_renovated', 'lat', 'long', 'sqft_living15', 'sqft_lot15']\n",
    "simple_housing.drop(columns=complexities, inplace=True)\n",
    "simple_housing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploratory Visualizations\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "fig = plt.figure(figsize = (10,10))\n",
    "ax = fig.gca()\n",
    "simple_housing.hist(ax = ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of our features suffer from some level of skewness and will need to be transformed to facilitate modelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatter matrix\n",
    "pd.plotting.scatter_matrix(simple_housing, figsize=(10,12));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation & Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we aim to maximize the value of our clients' investment, we do not believe home price is the most meaningful target variable. Certainly a higher sale price is appealing, but we must also consider the risk that comes with large individual investments. Expensive homes have a limited pool of buyers, even in a wealthy locale like King County. We will focus our analysis on the price per square foot of the properties in our data in order to allow our clients to make investment decisions that match their specific fund sizes and risk tolerances.\n",
    "\n",
    "We elect to drop the one instance of a building grade 3, as this code signifies that the property fails to meet minimum building standards and could be unsafe.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add price per square foot feature\n",
    "simple_housing['price_sqft'] = simple_housing['price'] / simple_housing['sqft_living']\n",
    "\n",
    "# drop components of price_sqft, and poor-graded structure & confirm no NaNs\n",
    "simple_housing = simple_housing.drop(columns=['price', 'sqft_living'])\n",
    "simple_housing = simple_housing[simple_housing['grade']!='3 Poor']\n",
    "\n",
    "# Map Missing values for waterfront to \"no\"\n",
    "# There are only 146 waterfront properties in the dataset, less than 1% of known data.\n",
    "simple_housing['waterfront'].fillna(value='NO', inplace=True)\n",
    "simple_housing.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the single variable most correlated with price_sqft (in this data frame it's yr_built).\n",
    "simple_housing.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/test split\n",
    "X = simple_housing.drop(columns=['price_sqft'])\n",
    "y = simple_housing['price_sqft']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=40, test_size=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode Categorical Variables in Test & Train data.\n",
    "\n",
    "# Identify catgeorical values & create new df with only the categorical columns.\n",
    "categoricals = ['condition', 'grade', 'waterfront', 'zipcode']\n",
    "X_train_cats = X_train[categoricals]\n",
    "X_test_cats = X_test[categoricals]\n",
    "\n",
    "# OneHot encoding for categorical variables in Train data. Drop first column to avoid perfect multicolinearity.\n",
    "ohe = OneHotEncoder(drop='first')\n",
    "X_train_cats_encoded = ohe.fit_transform(X_train_cats)\n",
    "X_train_cats_df = pd.DataFrame(X_train_cats_encoded.todense(), columns=ohe.get_feature_names(categoricals))\n",
    "\n",
    "# Drop original categorical columns, concatenate encoded columns.\n",
    "X_train_data = X_train.drop(columns=categoricals)\n",
    "X_train_encoded = pd.concat([X_train_data.reset_index(drop=True), X_train_cats_df.reset_index(drop=True)], axis=1)\n",
    "X_train_encoded.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# OneHot encoding for categorical variables in Test data.\n",
    "ohe = OneHotEncoder(drop='first')\n",
    "X_test_cats_encoded = ohe.fit_transform(X_test_cats)\n",
    "X_test_cats_df = pd.DataFrame(X_test_cats_encoded.todense(), columns=ohe.get_feature_names(categoricals))\n",
    "X_test_data = X_test.drop(columns=categoricals)\n",
    "X_test_encoded = pd.concat([X_test_data.reset_index(drop=True), X_test_cats_df.reset_index(drop=True)], axis=1)\n",
    "X_test_encoded.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Transformation\n",
    "We identified previously that our continuous variables all suffer from some level of skew, which we attempted to address with a log-transformation. The results of the transformation are visible in the below scatter matrix--bathrooms, bedrooms and sqft_lot are noticeably improved, while year built and floors remain skewed.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# log-transform non-normal features of Train Data\n",
    "non_normal = ['bathrooms', 'bedrooms', 'floors', 'sqft_lot', 'yr_built']\n",
    "train_preds_log = pd.DataFrame([])\n",
    "train_preds_log[non_normal] = np.log(X_train_encoded[non_normal])\n",
    "train_preds_log = pd.concat([train_preds_log.reset_index(drop=True), X_train_encoded.iloc[:,6:].reset_index(drop=True)], axis=1)\n",
    "\n",
    "# log-transform non-normal features of Test Data\n",
    "test_preds_log = pd.DataFrame([])\n",
    "test_preds_log[non_normal] = np.log(X_test_encoded[non_normal])\n",
    "test_preds_log = pd.concat([test_preds_log.reset_index(drop=True), X_test_encoded.iloc[:,6:].reset_index(drop=True)], axis=1)\n",
    "\n",
    "# Visualize Impact of Logged Train Preds\n",
    "pd.plotting.scatter_matrix(train_preds_log[non_normal], figsize=(10,12));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To facilitate easier comparison of our model coefficients we applied a standard scalar to our train and test data below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform scaling of our training data utilizing StandardScaler\n",
    "ss = StandardScaler()\n",
    "\n",
    "X_train_scaled = ss.fit_transform(train_preds_log)\n",
    "X_train_scaled_df = pd.DataFrame(X_train_scaled, index=train_preds_log.index, columns=train_preds_log.columns)\n",
    "\n",
    "# Perform scaling of our test data utilizing StandardScaler\n",
    "X_test_scaled = ss.fit_transform(test_preds_log)\n",
    "X_test_scaled_df = pd.DataFrame(X_test_scaled, index=test_preds_log.index, columns=test_preds_log.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our modelling efforts included a dummy regressor model, a simple linear regression model, and two multiple linear regression models. The models are presented in order from simplest to most complex to demonstrate our iterative process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dummy Regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin modelling efforts by creating a dummy regressor model to provide a baseline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Code Credit to geeksforgeeks.org. https://www.geeksforgeeks.org/dummy-regressor/\n",
    "lm = LinearRegression().fit(X_train_scaled, y_train)\n",
    "lm_dummy_mean = DummyRegressor(strategy = 'mean').fit(X_train_scaled, y_train) \n",
    "lm_dummy_median = DummyRegressor(strategy = 'median').fit(X_train_scaled, y_train)\n",
    "y_predict = lm.predict(X_test_scaled)\n",
    "y_predict_dummy_mean = lm_dummy_mean.predict(X_test_scaled)\n",
    "y_predict_dummy_median = lm_dummy_median.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mean squared error (dummy): {:.2f}\".format(mean_squared_error(y_test, y_predict_dummy_mean)))\n",
    "\n",
    "print(\"Median absolute error (dummy): {:.2f}\".format(median_absolute_error(y_test, y_predict_dummy_median)))\n",
    "\n",
    "print(\"r2_score (dummy mean): {:.2f}\".format(r2_score(y_test, y_predict_dummy_mean)))\n",
    "print(\"r2_score (dummy median): {:.2f}\".format(r2_score(y_test, y_predict_dummy_median)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Simple Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Among our initial set of features, the year in which the house was built was the most correlated variable with price per square foot. The negative correlation might mean that buyers in King county value older houses at a higher rate than new-builds. We began our analysis by modelling the log-scaled and standardized year in which the houses were constructed against the price per square foot. Unsurprisingly, the model is quite weak wtih an R<sup>2</sup> value of 0.012. We will move from here to a multiple linear regression model that considers all of the variables in our test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add constant term/y-intercept to train data\n",
    "X_train_const = sm.add_constant(X_train_scaled_df)\n",
    "X_test_const = sm.add_constant(X_test_scaled_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape y_train to facilitate modeling against X_train\n",
    "y_train=y_train.values.reshape(-1,1)\n",
    "model = sm.OLS(endog=y_train, exog=X_train_const['yr_built']).fit()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Multiple Linear Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our first multiple linear regression model, we elected to remove zip codes from the train/test data due to the large number of features created by one hot encoding the zips. We will explore their inclusion in subsequent models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove columns beginning with 'zip' from the train/test data and redefine our variables with 1, signifying the model.\n",
    "cols = [c for c in X_train_const if c.lower()[:3] != 'zip']\n",
    "X_train_const1 = X_train_const[cols]\n",
    "X_test_const1 = X_test_const[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First Multiple Linear Regression Model\n",
    "model = sm.OLS(endog=y_train, exog=X_train_const1).fit()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "R<sup>2</sup> value is much improved over our simple model, but still quite low. We will likely need to add additional features in order to improve the model. From our original dataset, zipcode is an appealing proxy for several attributes of a home, i.e. school district, proximity to amenities, etc. We will include the effect of zip code on price_sqft in our next model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Homoskedasticity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit Linear Regression object on the train data and scatter plot residuals.\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train_const1, y_train)\n",
    "\n",
    "y_hat = lr.predict(X_train_const1)\n",
    "resid = y_train - y_hat\n",
    "\n",
    "fig, ax=plt.subplots()\n",
    "ax.scatter(x=range(y_hat.shape[0]), y=resid, alpha=0.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Normality of Errors\n",
    "Creating a QQ plot and a histogram of the residuals revealed that although our transformations improved the normality of the data distribution, our train and test data contain outliers. Future models could explore removing the outliers in order to improve model accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preds = lr.predict(X_train_const1)\n",
    "test_preds = lr.predict(X_test_const1)\n",
    "\n",
    "# Calculate Residuals\n",
    "train_residuals = np.ndarray.flatten(y_train) - np.ndarray.flatten(train_preds)\n",
    "test_residuals = y_test - np.ndarray.flatten(test_preds)\n",
    "\n",
    "# QQ Plot -- Train\n",
    "sm.qqplot(train_residuals, line='r');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QQ Plot -- Test\n",
    "sm.qqplot(test_residuals, line='r');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of residuals\n",
    "plt.hist(train_residuals, label='Train')\n",
    "plt.hist(test_residuals, label='Test')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Multicolinearity\n",
    "Building grade 7 and grade 8 are above the general comfort level for VIF of 5. Future models could look to address this feature, although as categorical variables we cannot remove one or the other. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test for multicolinearity among features with VIF score\n",
    "\n",
    "vif = pd.DataFrame()\n",
    "vif[\"VIF\"] = [variance_inflation_factor(X_train_const1.values, i) for i in range(len(X_train_const1.columns))]\n",
    "vif[\"features\"] = X_train_const1.columns\n",
    "\n",
    "vif.sort_values(by='VIF', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first model performed slightly worse on the test than train data, but not enough to merit concern. Train/Test root mean squared errors appear quite high at 90.33 / 91.23 per square foot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit linear regression object on train data\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(X_train_const1, y_train)\n",
    "\n",
    "# Calculate predicted values for train and test data.\n",
    "y_pred_train = regressor.predict(X_train_const1)\n",
    "y_pred_test = regressor.predict(X_test_const1)\n",
    "\n",
    "# Calculate mean absolute and squared error and root mean squared error on TRAIN data\n",
    "train_mae = mean_absolute_error(y_train, y_pred_train)\n",
    "train_mse = mean_squared_error(y_train, y_pred_train)\n",
    "train_rmse = np.sqrt(train_mse)\n",
    "\n",
    "print(f'Train mean absolute error: {train_mae:.2f}')\n",
    "print(f'Train mean squared error: {train_mse:.2f}')\n",
    "print(f'Train root mean squared error: {train_rmse:.2f}')\n",
    "\n",
    "# Calculate mean absolute and squared error and root mean squared error on TEST data\n",
    "test_mae = mean_absolute_error(y_test, y_pred_test)\n",
    "test_mse = mean_squared_error(y_test, y_pred_test)\n",
    "test_rmse = np.sqrt(test_mse)\n",
    "\n",
    "print(f'Test mean absolute error: {test_mae:.2f}')\n",
    "print(f'Test mean squared error: {test_mse:.2f}')\n",
    "print(f'Test root mean squared error: {test_rmse:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2nd Linear Regression - Zip Codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Including the zip code data has dramatically improved the R<sup>2</sup> value of our model--slightly more than doubling it from 0.323 to 0.665. It appears that zip codes generally have a significant impact on the price per square foot of the properties in the data set.\n",
    "\n",
    "There are several large pvalues within the model, suggesting that number of floors, building grade 4, and certain zip codes may not have a stastically significant impact on the price per square foot of those properties. Given more time or resources we would look to investigate these features to better address their impact on the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Rerun multiple linear regression, this time including zip codes\n",
    "model = sm.OLS(endog=y_train, exog=X_train_const).fit()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking Model Assumptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Homoskedasticity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A scatter plot of our model's residuals did not reveal any concerns with heteroskedasticity. There is not a clearly observable cone pattern that would make us think our data's errors are not evenly distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Fit Linear Regression object on the train data and scatter plot residuals.\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train_const, y_train)\n",
    "\n",
    "y_hat = lr.predict(X_train_const)\n",
    "resid = y_train - y_hat\n",
    "\n",
    "fig, ax=plt.subplots()\n",
    "ax.scatter(x=range(y_hat.shape[0]), y=resid, alpha=0.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Normality\n",
    "Creating a QQ plot and a histogram of the residuals revealed that although our transformations improved the normality of the data distribution, our train and test data contain outliers. Future models could explore removing the outliers in order to improve model accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preds = lr.predict(X_train_const)\n",
    "test_preds = lr.predict(X_test_const)\n",
    "\n",
    "# Calculate Residuals\n",
    "train_residuals = np.ndarray.flatten(y_train) - np.ndarray.flatten(train_preds)\n",
    "test_residuals = y_test - np.ndarray.flatten(test_preds)\n",
    "\n",
    "# QQ Plot -- Train\n",
    "sm.qqplot(train_residuals, line='r');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QQ Plot -- Test\n",
    "sm.qqplot(test_residuals, line='r');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of residuals\n",
    "plt.hist(train_residuals, label='Train')\n",
    "plt.hist(test_residuals, label='Test')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Multicolinearity\n",
    "Building grade 7 and grade 8 remain problematic, and their impacts were magnified slightly in this model. Given additional time and resources, we would explore whether removing these features would improve model accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test for Multicolinearity\n",
    "\n",
    "vif = pd.DataFrame()\n",
    "vif[\"VIF\"] = [variance_inflation_factor(X_train_const.values, i) for i in range(len(X_train_const.columns))]\n",
    "vif[\"features\"] = X_train_const.columns\n",
    "\n",
    "vif.sort_values(by='VIF', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Model Validation / Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will discuss the performance and validity of our model, as well as the impact of certain features with the largest correlation coefficients. Note that some of the data referenced here appears below the written work, specifically the train/test comparisons of R<sup>2</sup> and RMSE.\n",
    "\n",
    "With an R<sup>2</sup> score of 0.665, we can say that our model captures ~66% of the variance in price per square foot that our features can collectively explain. As noted above, there are small issues with multicolinearity that could be addressed in future iterations to improve the model. Encouragingly, our model performed very similarly on our test and train data, with no significant difference in either  R<sup>2</sup> or RMSE scores. This suggests that the model is functioning with relatively low bias and low variance. While the absolute R<sup>2</sup> score leaves much to be desired, our model does not seem to be under or overfitted. Additional features could be added to improve R<sup>2</sup>.\n",
    "\n",
    "The root mean squared error score of 63.94 on the test data signifies that our model will generally have error of \\\\$63.94 per square foot of a property. Compared with the mean price per square foot in the test data of $264.29, a \\\\$63.94 error is significant, particularly when properties can be several thousand square feet. However, the RMSE is well within one standard deviation of the price per square foot in the test data, which measured \\\\$110.42. Comparisons to the train data were similar, with a root mean squared error score of 63.53, compared to mean price per square foot of \\\\$264.05 and a standard deviation of \\\\$109.78.\n",
    "\n",
    "Our 12 strongest coefficients (ranging from 37.01 for zip 98004 to 24.39 for zip 98119) are all related to the zip codes in which the properties are located, suggesting that the attributes of a zip code have significant impact on property value. Following these appealing zip codes, building grade 7, which was labeled 'average' carried the next largest impact on price per square foot, with a negative coefficient of -24.12. Perhaps unsurprisingly, the waterfront feature had a coefficient of 19.63, suggesting that a waterfront property can expect an increase of \\\\$19.63 per square foot.\n",
    "\n",
    "Based on our evaluation of model performance, we believe its use is warranted provided further effort goes into reducing outliers in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('R-squared score on train data: ' ,lr.score(X_train_const, y_train).round(4))\n",
    "print('R-squared score on test data: ' , lr.score(X_test_const, y_test).round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Fit linear regression object on train data\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(X_train_const, y_train)\n",
    "\n",
    "# Calculate predicted values for train and test data.\n",
    "y_pred_train = regressor.predict(X_train_const)\n",
    "y_pred_test = regressor.predict(X_test_const)\n",
    "\n",
    "# Calculate mean absolute and squared error and root mean squared error on TRAIN data\n",
    "train_mae = mean_absolute_error(y_train, y_pred_train)\n",
    "train_mse = mean_squared_error(y_train, y_pred_train)\n",
    "train_rmse = np.sqrt(train_mse)\n",
    "\n",
    "print(f'Train mean absolute error: {train_mae:.2f}')\n",
    "print(f'Train mean squared error: {train_mse:.2f}')\n",
    "print(f'Train root mean squared error: {train_rmse:.2f}')\n",
    "\n",
    "# Calculate mean absolute and squared error and root mean squared error on TEST data\n",
    "test_mae = mean_absolute_error(y_test, y_pred_test)\n",
    "test_mse = mean_squared_error(y_test, y_pred_test)\n",
    "test_rmse = np.sqrt(test_mse)\n",
    "\n",
    "print(f'Test mean absolute error: {test_mae:.2f}')\n",
    "print(f'Test mean squared error: {test_mse:.2f}')\n",
    "print(f'Test root mean squared error: {test_rmse:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate price_sqft mean and standard deviation in train/test data.\n",
    "print('Mean price per square foot in training data: ', y_train.mean().round(2))\n",
    "print('Mean price per square foot in test data: ', y_test.mean().round(2))\n",
    "\n",
    "print('Standard deviation of price per square foot in training data: ', y_train.std().round(2))\n",
    "print('Standard deviation of price per square foot in test data: ', y_test.std().round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following visualizations were created to aid in the presentation and highlight meaningful coefficients from our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe of correlation coefficients from Final Model.\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(X_train_const, y_train)\n",
    "model_coefficients = regressor.coef_.reshape(-1,1)\n",
    "feature_names = X_train_const.columns\n",
    "coefficients_df = pd.DataFrame(data = model_coefficients, index = feature_names, columns = ['Coefficient value'])\n",
    "coefficients_df2 = coefficients_df.sort_values(by='Coefficient value', ascending=False).head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "waterfront_coef = [19.63220]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe of mean data for zip codes with the eight highest correlation coefficients\n",
    "simple_means = simple_housing.groupby('zipcode').mean()\n",
    "target_codes = [98004, 98103, 98115, 98117, 98033, 98105, 98040, 98199] \n",
    "viz_zips = simple_means.loc[target_codes].reset_index()\n",
    "viz_zips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mean Price per Square Foot by Zip Code for Zips with Highest Impact on Price per Square Foot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create x and y axis label data\n",
    "zip_labels = list(viz_zips['zipcode'].apply(str))\n",
    "price_labels = list(viz_zips['price_sqft'].round(2))\n",
    "\n",
    "# Plot mean price/sqft for zip codes in zip labels\n",
    "plt.rcParams.update({'font.size': 15});\n",
    "x = np.arange(len(zip_labels))\n",
    "width = 0.35 \n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15,12))\n",
    "ax.set_ylabel('Price per Square Foot($)')\n",
    "ax.set_xlabel('Zip Code')\n",
    "ax.set_title('Mean Price per Square Foot by Zip Code')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(zip_labels)\n",
    "\n",
    "pps = ax.bar(zip_labels, price_labels, color='green')\n",
    "for p in pps:\n",
    "   height = p.get_height()\n",
    "   ax.annotate('{}'.format(height),\n",
    "      xy=(p.get_x() + p.get_width() / 2, height),\n",
    "      xytext=(0, 5), \n",
    "      textcoords=\"offset points\",  \n",
    "      ha='center', va='bottom')\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('../images/mean_psft_zip', facecolor='white', dpi=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation Coefficients for Zip Codes with Highest Impact on Price per Square Foot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create x and y axis label data\n",
    "zip_labels = list(viz_zips['zipcode'].apply(str))\n",
    "viz_coef = coefficients_df2['Coefficient value'].head(8).round(2).to_list()\n",
    "\n",
    "\n",
    "plt.rcParams.update({'font.size': 15});\n",
    "x = np.arange(len(zip_labels)) \n",
    "width = 0.35 \n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15,12))\n",
    "\n",
    "ax.set_ylabel('Correlation Coefficient')\n",
    "ax.set_xlabel('Zip Code')\n",
    "ax.set_title('Zip Codes Most Correlated with Increased Price per Square Foot')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(zip_labels)\n",
    "\n",
    "pps = ax.bar(zip_labels, viz_coef, color='green')\n",
    "for p in pps:\n",
    "   height = p.get_height()\n",
    "   ax.annotate('{}'.format(height),\n",
    "      xy=(p.get_x() + p.get_width() / 2, height),\n",
    "      xytext=(0, 5), # 3 points vertical offset\n",
    "      textcoords=\"offset points\",  \n",
    "      ha='center', va='bottom')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../images/zip_coefs', facecolor='white', dpi=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation Coefficients for Building Grades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create x and y axis label data\n",
    "\n",
    "grade_coefs =  {'4': 0.450,\n",
    "                '5': -4.17,\n",
    "                '6': -14.45,\n",
    "                '7': -24.12,\n",
    "                '8': -17.44,\n",
    "                '9': -6.35,\n",
    "                '11': 3.02,\n",
    "                '13': 3.19,\n",
    "                '12': 4.13}\n",
    "\n",
    "plt.rcParams.update({'font.size': 15});\n",
    "x = np.arange(len(grade_coefs.keys())) \n",
    "width = 0.35 \n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15,12))\n",
    "\n",
    "ax.set_ylabel('Correlation Coefficient')\n",
    "ax.set_xlabel('Building Grade')\n",
    "ax.set_title('Building Grade Correlation with Price per Square Foot')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(grade_coefs.keys())\n",
    "\n",
    "mask1 = y < 0\n",
    "mask2 = y >= 0\n",
    "pps = ax.bar(grade_coefs.keys(), grade_coefs.values(), color='green')\n",
    "\n",
    "for p in pps:\n",
    "    height = p.get_height()\n",
    "    if height > 0:\n",
    "        ax.annotate('{}'.format(height), xy=(p.get_x() + p.get_width() / 2, height), xytext=(0, 15),\n",
    "        textcoords=\"offset points\", ha='center')  \n",
    "        \n",
    "    else:\n",
    "        ax.annotate('{}'.format(height), xy=(p.get_x() + p.get_width() / 2, height), xytext=(0, -15),\n",
    "        textcoords=\"offset points\", ha='center')  # 3 points vertical offset\n",
    "plt.tight_layout()\n",
    "plt.savefig('../images/grade_coefs', dpi=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final Model Image for README"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code credit to sascha @ stackoverflow. https://stackoverflow.com/questions/46664082/python-how-to-save-statsmodels-results-as-image-file\n",
    "\n",
    "plt.rc('figure', figsize=(7, 7), facecolor='white')\n",
    "#plt.text(0.01, 0.05, str(model.summary()), {'fontsize': 12}) old approach\n",
    "plt.text(0.01, 0.05, str(model.summary()), {'fontsize': 10}, fontproperties = 'monospace') # approach improved by OP -> monospace!\n",
    "plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../images/model', bbox_inches = 'tight', facecolor='white', dpi=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefficients_df.sort_values(by = 'Coefficient value', ascending=False).head(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "Our final model performed similarly on the train and test data, and with an R<sup>2</sup> score of 0.66, it is approaching a usable model. Further iteration is needed to address small issues with multicolinearity and address the presence of outliers in both the train and test data. We identified the following features and their correlation coefficients that had the strongest positive or negative impacts on price per square foot in King County:\n",
    "\n",
    "* __Zip Codes 98004 (37.01), 98103 (35.27), 98115 (31.76), 98117 (30.79), 98112 (30.50),  98033 (26.22), 98105 (25.90), and 98040 (25.32).__ Considering that school districts, traffic patterns, proximity to amenities, crime, etc. all vary with zip code, it is reasonable to expect that the difference between our baseline zip code of 98001 and certain others will have a significant impact on property value. The baseline zip of 98001 is primarily associated with Auburn, WA, an inland suburb situated about halfway between Seattle and Tacoma. We chose this as a baseline because it is close enough to have access to Seattle-area amenities but far enough away to be more reasonably priced than some of the urban areas.\n",
    "\n",
    "* __Building Grades 7 (-24.12) and 8 (-17.44).__ While building grades are quite subjective, these two grades most closely define average quality of construction methods and materials. Each has a negative impact on price per square foot, with grade 7 and 8 having correlation coefficients of -24.12 and -17.44, respectively. One might assume that given the climate, proximity to salt water and steep hills present in King County, buyers prize quality materials and construction methods that will last in a hostile environment.\n",
    "\n",
    "* __Bedrooms (-20.82).__ All else being equal, each additional bedroom had a negative effect on price per square foot of \\\\$20.81. At first blush this is surprising, but square footage that is used for bedrooms is not being utilized for spaces that are more closely associated with home value such as kitchen and entertaining space. Further analysis would be needed to more fully explain the negative correlation of bedrooms and price per square foot in King County.\n",
    "\n",
    "* __Waterfront Property (19.63).__ Perhaps the least surprising finding in the study, a property's designation as waterfront had a positive impact on price per square foot. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendations\n",
    "\n",
    "Based on our findings, we recommend that:\n",
    "1. Focus property searches within the zip codes with most impact on price per square foot.\n",
    "2. Avoid properties built with average materials and techniques. \n",
    "3. Be thoughtful about the number of bedrooms in target properties. Unless you have requirements for a high number of bedrooms, it would be best to avoid properties that use space this way.\n",
    "4. To the extent waterfront property is available and priced appealingly, it should be the first stop for investors in King County."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
